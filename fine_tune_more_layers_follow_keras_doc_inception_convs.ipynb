{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"theano\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://keras.io/applications/\n",
    "- https://github.com/yang-zhang/courses/blob/scratch/deeplearning1/nbs/lesson2.ipynb\n",
    "- http://localhost:8887/notebooks/git/dog-breed-identification/fine_tune_2.ipynb\n",
    "- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Flatten\n",
    "from keras.models import Model, Sequential, load_model\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "from keras.applications import xception, inception_v3, imagenet_utils\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from secrets import KAGGLE_USER, KAGGLE_PW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "competition_name = 'dog-breed-identification'\n",
    "data_dir = '/opt/notebooks/data/' + competition_name + '/preprocessed'\n",
    "batch_size = 16\n",
    "target_size = (299, 299)\n",
    "nb_classes = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_data(data_dir, target_size=target_size, batch_size=batch_size):\n",
    "    gen = image.ImageDataGenerator()\n",
    "    batches = gen.flow_from_directory(data_dir+'/train', shuffle=False, target_size=target_size, batch_size=batch_size)\n",
    "    batches_val = gen.flow_from_directory(data_dir+'/valid', shuffle=False, target_size=target_size, batch_size=batch_size)\n",
    "    nb_batches = math.ceil(batches.n/batch_size)\n",
    "    nb_batches_val = math.ceil(batches_val.n/batch_size)\n",
    "    return batches, batches_val, nb_batches, nb_batches_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output of bottleneck model with preprocessed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add preprocessing at the front\n",
    "inputs = Input(shape=(299, 299, 3))\n",
    "x = Lambda(inception_v3.preprocess_input)(inputs)\n",
    "x = base_model(x)\n",
    "mdl_preprocess_base_model = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batches, batches_val, nb_batches, nb_batches_val = get_batch_data(data_dir)\n",
    "\n",
    "base_model_output = mdl_preprocess_base_model.predict_generator(batches, steps=nb_batches, verbose=1)\n",
    "np.save(data_dir+'/results/base_model_output_{}'.format(base_model.name) , base_model_output)\n",
    "\n",
    "base_model_output_val = model_base_model_output.predict_generator(batches_val, steps=nb_batches_val, verbose=1)\n",
    "np.save(data_dir+'/results/base_model_output_val_{}'.format(base_model.name) , base_model_output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model_output = np.load(data_dir+'/results/base_model_output_{}.npy'.format(base_model.name))\n",
    "base_model_output_val = np.load(data_dir+'/results/base_model_output_val_{}.npy'.format(base_model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune - 1 dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 282us/step - loss: 2.8805 - acc: 0.5733 - val_loss: 1.5655 - val_acc: 0.8320\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 1.0438 - acc: 0.8676 - val_loss: 0.7721 - val_acc: 0.8750\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.5677 - acc: 0.8980 - val_loss: 0.5253 - val_acc: 0.8830\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.3981 - acc: 0.9101 - val_loss: 0.4337 - val_acc: 0.8895\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.3185 - acc: 0.9194 - val_loss: 0.3803 - val_acc: 0.8960\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.2639 - acc: 0.9308 - val_loss: 0.3686 - val_acc: 0.8845\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.2312 - acc: 0.9377 - val_loss: 0.3539 - val_acc: 0.8890\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.2037 - acc: 0.9445 - val_loss: 0.3383 - val_acc: 0.8960\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.1806 - acc: 0.9517 - val_loss: 0.3318 - val_acc: 0.8910\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1637 - acc: 0.9541 - val_loss: 0.3305 - val_acc: 0.8955\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1483 - acc: 0.9624 - val_loss: 0.3216 - val_acc: 0.8940\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1361 - acc: 0.9638 - val_loss: 0.3297 - val_acc: 0.8935\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1222 - acc: 0.9689 - val_loss: 0.3317 - val_acc: 0.8910\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1137 - acc: 0.9721 - val_loss: 0.3253 - val_acc: 0.8920\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 8us/step - loss: 0.1061 - acc: 0.9736 - val_loss: 0.3246 - val_acc: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19ac8094a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_single_dense_on_base_model = Sequential(\n",
    "    [Dense(nb_classes, \n",
    "           activation='softmax', \n",
    "           input_shape=(base_model_output.shape[1],)\n",
    "          )]\n",
    ")\n",
    "mdl_single_dense_on_base_model.compile(optimizer=RMSprop(), \n",
    "                                       loss='categorical_crossentropy', \n",
    "                                       metrics=['accuracy'])\n",
    "\n",
    "batches, batches_val, nb_batches, nb_batches_val = get_batch_data(data_dir)\n",
    "y = to_categorical(batches.classes)\n",
    "y_val = to_categorical(batches_val.classes)\n",
    "\n",
    "mdl_single_dense_on_base_model.fit(base_model_output, \n",
    "                                   y, \n",
    "                                   epochs=15, \n",
    "                                   batch_size=nb_batches, \n",
    "                                   validation_data=(base_model_output_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune - 2 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8222/8222 [==============================] - 1s 76us/step - loss: 2.0527 - acc: 0.5766 - val_loss: 0.7766 - val_acc: 0.7890\n",
      "Epoch 2/5\n",
      "8222/8222 [==============================] - 0s 14us/step - loss: 0.5375 - acc: 0.8494 - val_loss: 0.5982 - val_acc: 0.8360\n",
      "Epoch 3/5\n",
      "8222/8222 [==============================] - 0s 14us/step - loss: 0.4130 - acc: 0.8797 - val_loss: 0.4530 - val_acc: 0.8625\n",
      "Epoch 4/5\n",
      "8222/8222 [==============================] - 0s 14us/step - loss: 0.3557 - acc: 0.8915 - val_loss: 0.5506 - val_acc: 0.8315\n",
      "Epoch 5/5\n",
      "8222/8222 [==============================] - 0s 14us/step - loss: 0.3150 - acc: 0.9033 - val_loss: 0.4628 - val_acc: 0.8640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19ac315fd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_2_dense_on_base_model = Sequential(\n",
    "    [Dense(1024, activation='relu', input_shape=(base_model_output.shape[1],)),\n",
    "    Dense(nb_classes, activation='softmax',)])\n",
    "\n",
    "mdl_2_dense_on_base_model.compile(optimizer=RMSprop(),\n",
    "                                  loss='categorical_crossentropy', \n",
    "                                  metrics=['accuracy'])\n",
    "\n",
    "batches, batches_val, nb_batches, nb_batches_val = get_batch_data(data_dir)\n",
    "y = to_categorical(batches.classes)\n",
    "y_val = to_categorical(batches_val.classes)\n",
    "\n",
    "mdl_2_dense_on_base_model.fit(base_model_output, \n",
    "                                   y, \n",
    "                                   epochs=5, \n",
    "                                   batch_size=nb_batches, \n",
    "                                   validation_data=(base_model_output_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune some conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# add preprocessing at the bottom\n",
    "inputs = Input(shape=(299, 299, 3))\n",
    "x = Lambda(inception_v3.preprocess_input)(inputs)\n",
    "x = base_model(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer \n",
    "predictions = Dense(120, activation='softmax')(x)\n",
    "# this is the model we will train\n",
    "model = Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow_from_directory(data_dir+'/train', shuffle=True, target_size=target_size, batch_size=batch_size)\n",
    "batches_val = gen.flow_from_directory(data_dir+'/valid', shuffle=False, target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "nb_batches = math.ceil(batches.n/batch_size)\n",
    "nb_batches_val = math.ceil(batches_val.n/batch_size)\n",
    "\n",
    "y_encode = batches.classes\n",
    "y_val_encode = batches_val.classes\n",
    "\n",
    "y = to_categorical(batches.classes)\n",
    "y_val = to_categorical(batches_val.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "514/514 [==============================] - 102s 199ms/step - loss: 1.9856 - acc: 0.5195 - val_loss: 0.8978 - val_acc: 0.7330\n",
      "Epoch 2/3\n",
      "514/514 [==============================] - 99s 193ms/step - loss: 1.0027 - acc: 0.7127 - val_loss: 0.8710 - val_acc: 0.7660\n",
      "Epoch 3/3\n",
      "514/514 [==============================] - 99s 193ms/step - loss: 0.8925 - acc: 0.7453 - val_loss: 0.7957 - val_acc: 0.7815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19087916d8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, \n",
    "                    steps_per_epoch=nb_batches, \n",
    "                    epochs=3,\n",
    "                    validation_data=batches_val,\n",
    "                    validation_steps=nb_batches_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_9\n",
      "1 conv2d_377\n",
      "2 batch_normalization_377\n",
      "3 activation_377\n",
      "4 conv2d_378\n",
      "5 batch_normalization_378\n",
      "6 activation_378\n",
      "7 conv2d_379\n",
      "8 batch_normalization_379\n",
      "9 activation_379\n",
      "10 max_pooling2d_17\n",
      "11 conv2d_380\n",
      "12 batch_normalization_380\n",
      "13 activation_380\n",
      "14 conv2d_381\n",
      "15 batch_normalization_381\n",
      "16 activation_381\n",
      "17 max_pooling2d_18\n",
      "18 conv2d_385\n",
      "19 batch_normalization_385\n",
      "20 activation_385\n",
      "21 conv2d_383\n",
      "22 conv2d_386\n",
      "23 batch_normalization_383\n",
      "24 batch_normalization_386\n",
      "25 activation_383\n",
      "26 activation_386\n",
      "27 average_pooling2d_37\n",
      "28 conv2d_382\n",
      "29 conv2d_384\n",
      "30 conv2d_387\n",
      "31 conv2d_388\n",
      "32 batch_normalization_382\n",
      "33 batch_normalization_384\n",
      "34 batch_normalization_387\n",
      "35 batch_normalization_388\n",
      "36 activation_382\n",
      "37 activation_384\n",
      "38 activation_387\n",
      "39 activation_388\n",
      "40 mixed0\n",
      "41 conv2d_392\n",
      "42 batch_normalization_392\n",
      "43 activation_392\n",
      "44 conv2d_390\n",
      "45 conv2d_393\n",
      "46 batch_normalization_390\n",
      "47 batch_normalization_393\n",
      "48 activation_390\n",
      "49 activation_393\n",
      "50 average_pooling2d_38\n",
      "51 conv2d_389\n",
      "52 conv2d_391\n",
      "53 conv2d_394\n",
      "54 conv2d_395\n",
      "55 batch_normalization_389\n",
      "56 batch_normalization_391\n",
      "57 batch_normalization_394\n",
      "58 batch_normalization_395\n",
      "59 activation_389\n",
      "60 activation_391\n",
      "61 activation_394\n",
      "62 activation_395\n",
      "63 mixed1\n",
      "64 conv2d_399\n",
      "65 batch_normalization_399\n",
      "66 activation_399\n",
      "67 conv2d_397\n",
      "68 conv2d_400\n",
      "69 batch_normalization_397\n",
      "70 batch_normalization_400\n",
      "71 activation_397\n",
      "72 activation_400\n",
      "73 average_pooling2d_39\n",
      "74 conv2d_396\n",
      "75 conv2d_398\n",
      "76 conv2d_401\n",
      "77 conv2d_402\n",
      "78 batch_normalization_396\n",
      "79 batch_normalization_398\n",
      "80 batch_normalization_401\n",
      "81 batch_normalization_402\n",
      "82 activation_396\n",
      "83 activation_398\n",
      "84 activation_401\n",
      "85 activation_402\n",
      "86 mixed2\n",
      "87 conv2d_404\n",
      "88 batch_normalization_404\n",
      "89 activation_404\n",
      "90 conv2d_405\n",
      "91 batch_normalization_405\n",
      "92 activation_405\n",
      "93 conv2d_403\n",
      "94 conv2d_406\n",
      "95 batch_normalization_403\n",
      "96 batch_normalization_406\n",
      "97 activation_403\n",
      "98 activation_406\n",
      "99 max_pooling2d_19\n",
      "100 mixed3\n",
      "101 conv2d_411\n",
      "102 batch_normalization_411\n",
      "103 activation_411\n",
      "104 conv2d_412\n",
      "105 batch_normalization_412\n",
      "106 activation_412\n",
      "107 conv2d_408\n",
      "108 conv2d_413\n",
      "109 batch_normalization_408\n",
      "110 batch_normalization_413\n",
      "111 activation_408\n",
      "112 activation_413\n",
      "113 conv2d_409\n",
      "114 conv2d_414\n",
      "115 batch_normalization_409\n",
      "116 batch_normalization_414\n",
      "117 activation_409\n",
      "118 activation_414\n",
      "119 average_pooling2d_40\n",
      "120 conv2d_407\n",
      "121 conv2d_410\n",
      "122 conv2d_415\n",
      "123 conv2d_416\n",
      "124 batch_normalization_407\n",
      "125 batch_normalization_410\n",
      "126 batch_normalization_415\n",
      "127 batch_normalization_416\n",
      "128 activation_407\n",
      "129 activation_410\n",
      "130 activation_415\n",
      "131 activation_416\n",
      "132 mixed4\n",
      "133 conv2d_421\n",
      "134 batch_normalization_421\n",
      "135 activation_421\n",
      "136 conv2d_422\n",
      "137 batch_normalization_422\n",
      "138 activation_422\n",
      "139 conv2d_418\n",
      "140 conv2d_423\n",
      "141 batch_normalization_418\n",
      "142 batch_normalization_423\n",
      "143 activation_418\n",
      "144 activation_423\n",
      "145 conv2d_419\n",
      "146 conv2d_424\n",
      "147 batch_normalization_419\n",
      "148 batch_normalization_424\n",
      "149 activation_419\n",
      "150 activation_424\n",
      "151 average_pooling2d_41\n",
      "152 conv2d_417\n",
      "153 conv2d_420\n",
      "154 conv2d_425\n",
      "155 conv2d_426\n",
      "156 batch_normalization_417\n",
      "157 batch_normalization_420\n",
      "158 batch_normalization_425\n",
      "159 batch_normalization_426\n",
      "160 activation_417\n",
      "161 activation_420\n",
      "162 activation_425\n",
      "163 activation_426\n",
      "164 mixed5\n",
      "165 conv2d_431\n",
      "166 batch_normalization_431\n",
      "167 activation_431\n",
      "168 conv2d_432\n",
      "169 batch_normalization_432\n",
      "170 activation_432\n",
      "171 conv2d_428\n",
      "172 conv2d_433\n",
      "173 batch_normalization_428\n",
      "174 batch_normalization_433\n",
      "175 activation_428\n",
      "176 activation_433\n",
      "177 conv2d_429\n",
      "178 conv2d_434\n",
      "179 batch_normalization_429\n",
      "180 batch_normalization_434\n",
      "181 activation_429\n",
      "182 activation_434\n",
      "183 average_pooling2d_42\n",
      "184 conv2d_427\n",
      "185 conv2d_430\n",
      "186 conv2d_435\n",
      "187 conv2d_436\n",
      "188 batch_normalization_427\n",
      "189 batch_normalization_430\n",
      "190 batch_normalization_435\n",
      "191 batch_normalization_436\n",
      "192 activation_427\n",
      "193 activation_430\n",
      "194 activation_435\n",
      "195 activation_436\n",
      "196 mixed6\n",
      "197 conv2d_441\n",
      "198 batch_normalization_441\n",
      "199 activation_441\n",
      "200 conv2d_442\n",
      "201 batch_normalization_442\n",
      "202 activation_442\n",
      "203 conv2d_438\n",
      "204 conv2d_443\n",
      "205 batch_normalization_438\n",
      "206 batch_normalization_443\n",
      "207 activation_438\n",
      "208 activation_443\n",
      "209 conv2d_439\n",
      "210 conv2d_444\n",
      "211 batch_normalization_439\n",
      "212 batch_normalization_444\n",
      "213 activation_439\n",
      "214 activation_444\n",
      "215 average_pooling2d_43\n",
      "216 conv2d_437\n",
      "217 conv2d_440\n",
      "218 conv2d_445\n",
      "219 conv2d_446\n",
      "220 batch_normalization_437\n",
      "221 batch_normalization_440\n",
      "222 batch_normalization_445\n",
      "223 batch_normalization_446\n",
      "224 activation_437\n",
      "225 activation_440\n",
      "226 activation_445\n",
      "227 activation_446\n",
      "228 mixed7\n",
      "229 conv2d_449\n",
      "230 batch_normalization_449\n",
      "231 activation_449\n",
      "232 conv2d_450\n",
      "233 batch_normalization_450\n",
      "234 activation_450\n",
      "235 conv2d_447\n",
      "236 conv2d_451\n",
      "237 batch_normalization_447\n",
      "238 batch_normalization_451\n",
      "239 activation_447\n",
      "240 activation_451\n",
      "241 conv2d_448\n",
      "242 conv2d_452\n",
      "243 batch_normalization_448\n",
      "244 batch_normalization_452\n",
      "245 activation_448\n",
      "246 activation_452\n",
      "247 max_pooling2d_20\n",
      "248 mixed8\n",
      "249 conv2d_457\n",
      "250 batch_normalization_457\n",
      "251 activation_457\n",
      "252 conv2d_454\n",
      "253 conv2d_458\n",
      "254 batch_normalization_454\n",
      "255 batch_normalization_458\n",
      "256 activation_454\n",
      "257 activation_458\n",
      "258 conv2d_455\n",
      "259 conv2d_456\n",
      "260 conv2d_459\n",
      "261 conv2d_460\n",
      "262 average_pooling2d_44\n",
      "263 conv2d_453\n",
      "264 batch_normalization_455\n",
      "265 batch_normalization_456\n",
      "266 batch_normalization_459\n",
      "267 batch_normalization_460\n",
      "268 conv2d_461\n",
      "269 batch_normalization_453\n",
      "270 activation_455\n",
      "271 activation_456\n",
      "272 activation_459\n",
      "273 activation_460\n",
      "274 batch_normalization_461\n",
      "275 activation_453\n",
      "276 mixed9_0\n",
      "277 concatenate_9\n",
      "278 activation_461\n",
      "279 mixed9\n",
      "280 conv2d_466\n",
      "281 batch_normalization_466\n",
      "282 activation_466\n",
      "283 conv2d_463\n",
      "284 conv2d_467\n",
      "285 batch_normalization_463\n",
      "286 batch_normalization_467\n",
      "287 activation_463\n",
      "288 activation_467\n",
      "289 conv2d_464\n",
      "290 conv2d_465\n",
      "291 conv2d_468\n",
      "292 conv2d_469\n",
      "293 average_pooling2d_45\n",
      "294 conv2d_462\n",
      "295 batch_normalization_464\n",
      "296 batch_normalization_465\n",
      "297 batch_normalization_468\n",
      "298 batch_normalization_469\n",
      "299 conv2d_470\n",
      "300 batch_normalization_462\n",
      "301 activation_464\n",
      "302 activation_465\n",
      "303 activation_468\n",
      "304 activation_469\n",
      "305 batch_normalization_470\n",
      "306 activation_462\n",
      "307 mixed9_1\n",
      "308 concatenate_10\n",
      "309 activation_470\n",
      "310 mixed10\n",
      "311 global_average_pooling2d_5\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in base_model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in base_model.layers[249:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 119s 232ms/step - loss: 0.5236 - acc: 0.8357 - val_loss: 0.6446 - val_acc: 0.8075\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_0_2017-11-16-14-46.h5\n",
      "Running epoch: 1\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.3941 - acc: 0.8781 - val_loss: 0.6264 - val_acc: 0.8080\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_1_2017-11-16-14-48.h5\n",
      "Running epoch: 2\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.3463 - acc: 0.8930 - val_loss: 0.6258 - val_acc: 0.8120\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_2_2017-11-16-14-50.h5\n",
      "Running epoch: 3\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.3006 - acc: 0.9073 - val_loss: 0.6226 - val_acc: 0.8140\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_3_2017-11-16-14-52.h5\n",
      "Running epoch: 4\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.2781 - acc: 0.9160 - val_loss: 0.6207 - val_acc: 0.8155\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_4_2017-11-16-14-54.h5\n",
      "Running epoch: 5\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.2534 - acc: 0.9231 - val_loss: 0.6189 - val_acc: 0.8145\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_5_2017-11-16-14-56.h5\n",
      "Running epoch: 6\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.2313 - acc: 0.9337 - val_loss: 0.6166 - val_acc: 0.8170\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_6_2017-11-16-14-58.h5\n",
      "Running epoch: 7\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.2045 - acc: 0.9432 - val_loss: 0.6222 - val_acc: 0.8175\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_7_2017-11-16-15-00.h5\n",
      "Running epoch: 8\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1956 - acc: 0.9455 - val_loss: 0.6231 - val_acc: 0.8175\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_8_2017-11-16-15-02.h5\n",
      "Running epoch: 9\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1782 - acc: 0.9504 - val_loss: 0.6245 - val_acc: 0.8160\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_9_2017-11-16-15-04.h5\n",
      "Running epoch: 10\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1631 - acc: 0.9583 - val_loss: 0.6267 - val_acc: 0.8155\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_10_2017-11-16-15-06.h5\n",
      "Running epoch: 11\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1580 - acc: 0.9605 - val_loss: 0.6232 - val_acc: 0.8170\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_11_2017-11-16-15-08.h5\n",
      "Running epoch: 12\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1440 - acc: 0.9647 - val_loss: 0.6271 - val_acc: 0.8215\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_12_2017-11-16-15-10.h5\n",
      "Running epoch: 13\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1284 - acc: 0.9696 - val_loss: 0.6259 - val_acc: 0.8220\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_13_2017-11-16-15-12.h5\n",
      "Running epoch: 14\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1236 - acc: 0.9723 - val_loss: 0.6265 - val_acc: 0.8250\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_14_2017-11-16-15-14.h5\n",
      "Running epoch: 15\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1157 - acc: 0.9736 - val_loss: 0.6282 - val_acc: 0.8225\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_15_2017-11-16-15-15.h5\n",
      "Running epoch: 16\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1116 - acc: 0.9772 - val_loss: 0.6303 - val_acc: 0.8230\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_16_2017-11-16-15-17.h5\n",
      "Running epoch: 17\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.1070 - acc: 0.9790 - val_loss: 0.6326 - val_acc: 0.8240\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_17_2017-11-16-15-19.h5\n",
      "Running epoch: 18\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0975 - acc: 0.9805 - val_loss: 0.6335 - val_acc: 0.8225\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_18_2017-11-16-15-21.h5\n",
      "Running epoch: 19\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0988 - acc: 0.9776 - val_loss: 0.6236 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_19_2017-11-16-15-23.h5\n",
      "Running epoch: 20\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0933 - acc: 0.9807 - val_loss: 0.6315 - val_acc: 0.8245\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_20_2017-11-16-15-25.h5\n",
      "Running epoch: 21\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0866 - acc: 0.9832 - val_loss: 0.6361 - val_acc: 0.8225\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_21_2017-11-16-15-27.h5\n",
      "Running epoch: 22\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0866 - acc: 0.9839 - val_loss: 0.6402 - val_acc: 0.8235\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_22_2017-11-16-15-29.h5\n",
      "Running epoch: 23\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0774 - acc: 0.9849 - val_loss: 0.6360 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_23_2017-11-16-15-31.h5\n",
      "Running epoch: 24\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0749 - acc: 0.9866 - val_loss: 0.6336 - val_acc: 0.8270\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_24_2017-11-16-15-33.h5\n",
      "Running epoch: 25\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0671 - acc: 0.9889 - val_loss: 0.6373 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_25_2017-11-16-15-35.h5\n",
      "Running epoch: 26\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0651 - acc: 0.9882 - val_loss: 0.6449 - val_acc: 0.8260\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_26_2017-11-16-15-37.h5\n",
      "Running epoch: 27\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0693 - acc: 0.9859 - val_loss: 0.6434 - val_acc: 0.8280\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_27_2017-11-16-15-39.h5\n",
      "Running epoch: 28\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0686 - acc: 0.9865 - val_loss: 0.6347 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_28_2017-11-16-15-41.h5\n",
      "Running epoch: 29\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0631 - acc: 0.9893 - val_loss: 0.6359 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_29_2017-11-16-15-43.h5\n",
      "Running epoch: 30\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0620 - acc: 0.9888 - val_loss: 0.6370 - val_acc: 0.8305\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_30_2017-11-16-15-44.h5\n",
      "Running epoch: 31\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0559 - acc: 0.9923 - val_loss: 0.6455 - val_acc: 0.8250\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_31_2017-11-16-15-46.h5\n",
      "Running epoch: 32\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 116s 226ms/step - loss: 0.0586 - acc: 0.9902 - val_loss: 0.6496 - val_acc: 0.8275\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_32_2017-11-16-15-48.h5\n",
      "Running epoch: 33\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0518 - acc: 0.9908 - val_loss: 0.6506 - val_acc: 0.8250\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_33_2017-11-16-15-50.h5\n",
      "Running epoch: 34\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0514 - acc: 0.9909 - val_loss: 0.6482 - val_acc: 0.8245\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_34_2017-11-16-15-52.h5\n",
      "Running epoch: 35\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0489 - acc: 0.9929 - val_loss: 0.6499 - val_acc: 0.8265\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_35_2017-11-16-15-54.h5\n",
      "Running epoch: 36\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0506 - acc: 0.9928 - val_loss: 0.6517 - val_acc: 0.8250\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_36_2017-11-16-15-56.h5\n",
      "Running epoch: 37\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0452 - acc: 0.9928 - val_loss: 0.6444 - val_acc: 0.8295\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_37_2017-11-16-15-58.h5\n",
      "Running epoch: 38\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0473 - acc: 0.9931 - val_loss: 0.6500 - val_acc: 0.8270\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_38_2017-11-16-16-00.h5\n",
      "Running epoch: 39\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0459 - acc: 0.9935 - val_loss: 0.6557 - val_acc: 0.8240\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_39_2017-11-16-16-02.h5\n",
      "Running epoch: 40\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 115s 224ms/step - loss: 0.0414 - acc: 0.9944 - val_loss: 0.6594 - val_acc: 0.8245\n",
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_40_2017-11-16-16-04.h5\n",
      "Running epoch: 41\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Epoch 1/1\n",
      "115/514 [=====>........................] - ETA: 1:13 - loss: 0.0381 - acc: 0.9951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-943ee0e9588f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_batches_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                    )\n\u001b[1;32m     14\u001b[0m     latest_filename = data_dir+'/results/ft_top_%d_%s.h5' %  (epoch,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2112\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2113\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "no_of_epochs = 50\n",
    "for epoch in range(no_of_epochs):\n",
    "    print (\"Running epoch: %d\" % epoch)\n",
    "    batches = gen.flow_from_directory(data_dir+'/train', shuffle=True, target_size=target_size, batch_size=batch_size)\n",
    "    batches_val = gen.flow_from_directory(data_dir+'/valid', shuffle=False, target_size=target_size, batch_size=batch_size)\n",
    "    model.fit_generator(batches, \n",
    "                    steps_per_epoch=nb_batches, \n",
    "                    epochs=1,\n",
    "                    validation_data=batches_val,\n",
    "                    validation_steps=nb_batches_val\n",
    "                   )\n",
    "    latest_filename = data_dir+'/results/ft_top_%d_%s.h5' %  (epoch,\n",
    "        datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    )\n",
    "    print(latest_filename)\n",
    "    model.save(latest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_5_2017-11-16-14-56.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls /opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_5_2017-11-16-14-56.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imagenet_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b1f5498dcc34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_5_2017-11-16-14-56.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    138\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 139\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m                         \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   2455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2457\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2458\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mPreprocessed\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \"\"\"\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imagenet_utils' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('/opt/notebooks/data/dog-breed-identification/preprocessed/results/ft_top_5_2017-11-16-14-56.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10357 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches_test = gen.flow_from_directory(data_dir+'/test', shuffle=False, target_size=target_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_batches_test = math.ceil(batches_test.n/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648/648 [==============================] - 117s 180ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_generator(batches_test, steps=nb_batches_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = [f.split('/')[1].split('.')[0] for f in batches_test.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm=pd.DataFrame(np.hstack([np.array(test_ids).reshape(-1, 1), pred]))\n",
    "labels = pd.read_csv(data_dir+'/labels.csv')\n",
    "cols = ['id']+sorted(labels.breed.unique())\n",
    "subm.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "description = 'inception_data_finetune_more_layers'\n",
    "submission_file_name = data_dir+'/results/%s_%s.csv' % (description,\n",
    "                                                        datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "                                                       )\n",
    "subm.to_csv(submission_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kg config -u $KAGGLE_USER -p $KAGGLE_PW -c $competition_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65763\r\n"
     ]
    }
   ],
   "source": [
    "!kg submit $submission_file_name -m $description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "594px",
    "left": "0px",
    "right": "1064px",
    "top": "106px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
