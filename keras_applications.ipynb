{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "from secrets import KAGGLE_USER, KAGGLE_PW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "competition_name = 'dog-breed-identification'\n",
    "data_dir = '/opt/notebooks/data/' + competition_name + '/preprocessed'\n",
    "batch_size = 16\n",
    "nb_classes = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_xception(x):\n",
    "    return imagenet_utils.preprocess_input(x, mode='tf')\n",
    "\n",
    "def preprocess_input_vgg(x):\n",
    "    vgg_mean = np.array([103.939, 116.779, 123.68], dtype=np.float32).reshape((1,1,3))\n",
    "    x = x[..., ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    x = x - vgg_mean\n",
    "    return x\n",
    "\n",
    "def add_preprocess(base_model, preprocess_func, inputs_shape):\n",
    "    inputs = Input(shape=inputs_shape)\n",
    "    x = Lambda(preprocess_func)(inputs)\n",
    "    outputs = base_model(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_batch_data(data_dir, target_size):\n",
    "    \n",
    "    def get_batches(dir_, target_size=target_size):\n",
    "        gen=image.ImageDataGenerator()\n",
    "        return gen.flow_from_directory('%s/%s'% (data_dir, dir_), \n",
    "                                       shuffle=False, \n",
    "                                       target_size=target_size,\n",
    "                                       batch_size=batch_size)\n",
    "    batches     = get_batches('train')\n",
    "    batches_val = get_batches('valid')\n",
    "    batches_test = get_batches('test')\n",
    "    return batches, batches_val, batches_test\n",
    "\n",
    "def get_batch_nb(batches):\n",
    "    return math.ceil(batches.n/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params = {'weights': 'imagenet', 'include_top': False, 'pooling': 'avg'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    {'name': 'Xception', 'mdl': Xception, 'input_shape': (299, 299, 3), 'prep': preprocess_input_xception},\n",
    "    {'name': 'VGG16', 'mdl': VGG16, 'input_shape': (224, 224, 3), 'prep': preprocess_input_vgg},\n",
    "    {'name': 'VGG19', 'mdl': VGG19, 'input_shape': (224, 224, 3), 'prep': preprocess_input_vgg},\n",
    "    {'name': 'InceptionV3', 'mdl': InceptionV3, 'input_shape': (299, 299, 3), 'prep': preprocess_input_xception},\n",
    "    {'name': 'ResNet50', 'mdl': ResNet50, 'input_shape': (224, 224, 3), 'prep': preprocess_input_vgg},\n",
    "    {'name': 'InceptionResNetV2', 'mdl': InceptionResNetV2, 'input_shape': (299, 299, 3), 'prep': preprocess_input_xception},\n",
    "#     {'mdl': MobileNet, 'input_shape': (299, 299, 3), 'prep': preprocess_input_vgg},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 127s 247ms/step\n",
      "125/125 [==============================] - 31s 247ms/step\n",
      "648/648 [==============================] - 159s 246ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 186us/step - loss: 3.0564 - acc: 0.6789 - val_loss: 1.8896 - val_acc: 0.8450\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 1.3502 - acc: 0.8814 - val_loss: 0.9808 - val_acc: 0.8895\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.7463 - acc: 0.9004 - val_loss: 0.6370 - val_acc: 0.8945\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.5049 - acc: 0.9104 - val_loss: 0.4851 - val_acc: 0.9025\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.3887 - acc: 0.9181 - val_loss: 0.4102 - val_acc: 0.8955\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.3213 - acc: 0.9240 - val_loss: 0.3654 - val_acc: 0.9060\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.2780 - acc: 0.9320 - val_loss: 0.3438 - val_acc: 0.9055\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2475 - acc: 0.9360 - val_loss: 0.3291 - val_acc: 0.9065\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2234 - acc: 0.9406 - val_loss: 0.3194 - val_acc: 0.9095\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 9us/step - loss: 0.2041 - acc: 0.9464 - val_loss: 0.3133 - val_acc: 0.9055\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1891 - acc: 0.9505 - val_loss: 0.3065 - val_acc: 0.9075\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1747 - acc: 0.9531 - val_loss: 0.3018 - val_acc: 0.9055\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1628 - acc: 0.9557 - val_loss: 0.3030 - val_acc: 0.9095\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1518 - acc: 0.9595 - val_loss: 0.3006 - val_acc: 0.9080\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1426 - acc: 0.9623 - val_loss: 0.2990 - val_acc: 0.9115\n",
      "10357/10357 [==============================] - 1s 125us/step\n",
      "VGG16\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 75s 145ms/step\n",
      "125/125 [==============================] - 18s 142ms/step\n",
      "648/648 [==============================] - 92s 142ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 184us/step - loss: 12.2188 - acc: 0.0478 - val_loss: 10.1840 - val_acc: 0.0865\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 8.7661 - acc: 0.1497 - val_loss: 8.0573 - val_acc: 0.1800\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 7.0333 - acc: 0.2655 - val_loss: 7.0955 - val_acc: 0.2595\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 6.0722 - acc: 0.3478 - val_loss: 6.4235 - val_acc: 0.3080\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 5.3613 - acc: 0.4185 - val_loss: 5.9808 - val_acc: 0.3435\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 4.8605 - acc: 0.4724 - val_loss: 5.5826 - val_acc: 0.3665\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 4.3779 - acc: 0.5171 - val_loss: 5.3535 - val_acc: 0.3965\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 4.0363 - acc: 0.5586 - val_loss: 5.1331 - val_acc: 0.4045\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.8150 - acc: 0.6014 - val_loss: 4.9972 - val_acc: 0.4340\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.6522 - acc: 0.6287 - val_loss: 4.9507 - val_acc: 0.4385\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.5313 - acc: 0.6613 - val_loss: 4.8794 - val_acc: 0.4535\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.4219 - acc: 0.6772 - val_loss: 4.8739 - val_acc: 0.4570\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.3400 - acc: 0.7014 - val_loss: 4.8537 - val_acc: 0.4585\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.2737 - acc: 0.7180 - val_loss: 4.7883 - val_acc: 0.4800\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.2088 - acc: 0.7345 - val_loss: 4.7913 - val_acc: 0.4755\n",
      "10357/10357 [==============================] - 1s 121us/step\n",
      "VGG19\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 88s 171ms/step\n",
      "125/125 [==============================] - 21s 171ms/step\n",
      "648/648 [==============================] - 110s 170ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 187us/step - loss: 11.5266 - acc: 0.0530 - val_loss: 9.4102 - val_acc: 0.0935\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 8.1163 - acc: 0.1653 - val_loss: 7.4524 - val_acc: 0.1920\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 6.3518 - acc: 0.2711 - val_loss: 6.2421 - val_acc: 0.2555\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 5.2477 - acc: 0.3623 - val_loss: 5.5225 - val_acc: 0.3095\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 4.5269 - acc: 0.4347 - val_loss: 5.1056 - val_acc: 0.3610\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 4.0704 - acc: 0.4893 - val_loss: 4.8158 - val_acc: 0.3880\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.7412 - acc: 0.5411 - val_loss: 4.5374 - val_acc: 0.4170\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.4955 - acc: 0.5752 - val_loss: 4.4067 - val_acc: 0.4375\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.2716 - acc: 0.6182 - val_loss: 4.3227 - val_acc: 0.4510\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.1347 - acc: 0.6396 - val_loss: 4.2832 - val_acc: 0.4595\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 3.0166 - acc: 0.6734 - val_loss: 4.2246 - val_acc: 0.4720\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 2.9085 - acc: 0.6965 - val_loss: 4.1968 - val_acc: 0.4805\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 2.8165 - acc: 0.7164 - val_loss: 4.1320 - val_acc: 0.4785\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 2.7395 - acc: 0.7413 - val_loss: 4.1247 - val_acc: 0.4890\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 7us/step - loss: 2.6897 - acc: 0.7488 - val_loss: 4.1172 - val_acc: 0.4845\n",
      "10357/10357 [==============================] - 1s 119us/step\n",
      "InceptionV3\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 87s 169ms/step\n",
      "125/125 [==============================] - 20s 163ms/step\n",
      "648/648 [==============================] - 105s 163ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 234us/step - loss: 2.8088 - acc: 0.5896 - val_loss: 1.5114 - val_acc: 0.8285\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 1.0096 - acc: 0.8766 - val_loss: 0.7554 - val_acc: 0.8690\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.5545 - acc: 0.9004 - val_loss: 0.5235 - val_acc: 0.8780\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.3940 - acc: 0.9141 - val_loss: 0.4368 - val_acc: 0.8800\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.3141 - acc: 0.9252 - val_loss: 0.3891 - val_acc: 0.8865\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2635 - acc: 0.9315 - val_loss: 0.3714 - val_acc: 0.8870\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2272 - acc: 0.9405 - val_loss: 0.3541 - val_acc: 0.8875\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2018 - acc: 0.9461 - val_loss: 0.3422 - val_acc: 0.8905\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1825 - acc: 0.9484 - val_loss: 0.3382 - val_acc: 0.8875\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1629 - acc: 0.9573 - val_loss: 0.3336 - val_acc: 0.8915\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1462 - acc: 0.9633 - val_loss: 0.3383 - val_acc: 0.8880\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1344 - acc: 0.9647 - val_loss: 0.3264 - val_acc: 0.8950\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1242 - acc: 0.9663 - val_loss: 0.3305 - val_acc: 0.8925\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1122 - acc: 0.9719 - val_loss: 0.3303 - val_acc: 0.8900\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1033 - acc: 0.9749 - val_loss: 0.3245 - val_acc: 0.8945\n",
      "10357/10357 [==============================] - 2s 150us/step\n",
      "ResNet50\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 66s 129ms/step\n",
      "125/125 [==============================] - 16s 126ms/step\n",
      "648/648 [==============================] - 81s 125ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 2s 261us/step - loss: 3.3808 - acc: 0.2651 - val_loss: 2.1879 - val_acc: 0.4910\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 1.6012 - acc: 0.6450 - val_loss: 1.5159 - val_acc: 0.6215\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 1.0702 - acc: 0.7499 - val_loss: 1.2389 - val_acc: 0.6640\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.7891 - acc: 0.8127 - val_loss: 1.1193 - val_acc: 0.6850\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.6265 - acc: 0.8481 - val_loss: 1.0274 - val_acc: 0.6955\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.4983 - acc: 0.8819 - val_loss: 0.9886 - val_acc: 0.7070\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.4065 - acc: 0.9073 - val_loss: 0.9467 - val_acc: 0.7160\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.3435 - acc: 0.9245 - val_loss: 0.9323 - val_acc: 0.7175\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.2825 - acc: 0.9458 - val_loss: 0.9156 - val_acc: 0.7260\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.2365 - acc: 0.9597 - val_loss: 0.9284 - val_acc: 0.7235\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1979 - acc: 0.9683 - val_loss: 0.9144 - val_acc: 0.7235\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1689 - acc: 0.9747 - val_loss: 0.9149 - val_acc: 0.7245\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1425 - acc: 0.9825 - val_loss: 0.9366 - val_acc: 0.7135\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1193 - acc: 0.9880 - val_loss: 0.9146 - val_acc: 0.7220\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 10us/step - loss: 0.1027 - acc: 0.9912 - val_loss: 0.9145 - val_acc: 0.7240\n",
      "10357/10357 [==============================] - 2s 165us/step\n",
      "InceptionResNetV2\n",
      "Found 8222 images belonging to 120 classes.\n",
      "Found 2000 images belonging to 120 classes.\n",
      "Found 10357 images belonging to 1 classes.\n",
      "514/514 [==============================] - 156s 304ms/step\n",
      "125/125 [==============================] - 37s 297ms/step\n",
      "648/648 [==============================] - 192s 296ms/step\n",
      "Train on 8222 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8222/8222 [==============================] - 3s 356us/step - loss: 2.7192 - acc: 0.7058 - val_loss: 1.3592 - val_acc: 0.8845\n",
      "Epoch 2/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.8705 - acc: 0.9003 - val_loss: 0.5905 - val_acc: 0.9035\n",
      "Epoch 3/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.4455 - acc: 0.9146 - val_loss: 0.3975 - val_acc: 0.9110\n",
      "Epoch 4/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.3223 - acc: 0.9209 - val_loss: 0.3311 - val_acc: 0.9120\n",
      "Epoch 5/15\n",
      "8222/8222 [==============================] - 0s 12us/step - loss: 0.2699 - acc: 0.9251 - val_loss: 0.3076 - val_acc: 0.9125\n",
      "Epoch 6/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.2395 - acc: 0.9295 - val_loss: 0.2988 - val_acc: 0.9095\n",
      "Epoch 7/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.2210 - acc: 0.9336 - val_loss: 0.2896 - val_acc: 0.9180\n",
      "Epoch 8/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.2053 - acc: 0.9353 - val_loss: 0.2872 - val_acc: 0.9120\n",
      "Epoch 9/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1927 - acc: 0.9389 - val_loss: 0.2935 - val_acc: 0.9095\n",
      "Epoch 10/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1827 - acc: 0.9421 - val_loss: 0.2902 - val_acc: 0.9130\n",
      "Epoch 11/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1739 - acc: 0.9441 - val_loss: 0.2872 - val_acc: 0.9150\n",
      "Epoch 12/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1657 - acc: 0.9475 - val_loss: 0.2915 - val_acc: 0.9140\n",
      "Epoch 13/15\n",
      "8222/8222 [==============================] - 0s 12us/step - loss: 0.1594 - acc: 0.9483 - val_loss: 0.2965 - val_acc: 0.9080\n",
      "Epoch 14/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1529 - acc: 0.9506 - val_loss: 0.2979 - val_acc: 0.9110\n",
      "Epoch 15/15\n",
      "8222/8222 [==============================] - 0s 11us/step - loss: 0.1464 - acc: 0.9528 - val_loss: 0.2936 - val_acc: 0.9140\n",
      "10357/10357 [==============================] - 2s 219us/step\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(base_model['name'])\n",
    "    base_model_preprocessed = add_preprocess(\n",
    "        base_model=base_model['mdl'](**model_params), \n",
    "        preprocess_func=base_model['prep'], \n",
    "        inputs_shape=base_model['input_shape'],\n",
    "    )\n",
    "\n",
    "    batches, batches_val, batches_test = \\\n",
    "        get_batch_data(data_dir, \n",
    "                       target_size=base_model['input_shape'][:2],)\n",
    "    nb_batches = get_batch_nb(batches)\n",
    "    nb_batches_val = get_batch_nb(batches_val) \n",
    "    nb_batches_test = get_batch_nb(batches_test)\n",
    "    \n",
    "    # get bottleneck features\n",
    "\n",
    "#     base_model_output = base_model_preprocessed.predict_generator(batches, \n",
    "#                                                                   steps=nb_batches, \n",
    "#                                                                   verbose=1)\n",
    "#     np.save(data_dir+'/results/base_model_output_{}'.format(base_model['name']), \n",
    "#             base_model_output)\n",
    "    base_model_output = np.load(data_dir+'/results/base_model_output_{}.npy'.format(base_model['name']))\n",
    "    \n",
    "#     base_model_output_val = base_model_preprocessed.predict_generator(batches_val, \n",
    "#                                                                       steps=nb_batches_val, \n",
    "#                                                                       verbose=1)\n",
    "#     np.save(data_dir+'/results/base_model_output_val_{}'.format(base_model['name']), \n",
    "#             base_model_output_val)\n",
    "    base_model_output_val = np.load(data_dir+'/results/base_model_output_val_{}.npy'.format(base_model['name']))\n",
    "\n",
    "#     base_model_output_test = base_model_preprocessed.predict_generator(batches_test, \n",
    "#                                                                       steps=nb_batches_test, \n",
    "#                                                                       verbose=1)\n",
    "#     np.save(data_dir+'/results/base_model_output_test_{}'.format(base_model['name']), \n",
    "#             base_model_output_test)\n",
    "    base_model_output_test = np.load(data_dir+'/results/base_model_output_test_{}.npy'.format(base_model['name']))\n",
    "\n",
    "    # linear model\n",
    "    lm = Sequential(\n",
    "        [Dense(nb_classes, \n",
    "               activation='softmax', \n",
    "               input_shape=(base_model_output.shape[1],)\n",
    "              )]\n",
    "    )\n",
    "    lm.compile(optimizer='rmsprop',\n",
    "               loss='categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "    y = to_categorical(batches.classes)\n",
    "    y_val = to_categorical(batches_val.classes)\n",
    "    lm.fit(base_model_output,\n",
    "           y, \n",
    "           epochs=15,\n",
    "           batch_size=nb_batches,\n",
    "           validation_data=(base_model_output_val, y_val))\n",
    "\n",
    "    pred = lm.predict(base_model_output_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    preds.append(pred)\n",
    "\n",
    "pred_ensemble = np.stack(preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ensemble = np.stack(np.array(preds)[[0,3,5]]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = [f.split('/')[1].split('.')[0] for f in batches_test.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm=pd.DataFrame(np.hstack([np.array(test_ids).reshape(-1, 1), pred_ensemble]))\n",
    "labels = pd.read_csv(data_dir+'/labels.csv')\n",
    "cols = ['id']+sorted(labels.breed.unique())\n",
    "subm.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'xception_inception_inception_resnet_average'\n",
    "submission_file_name = data_dir+'/results/%s_%s.csv' % (description,\n",
    "                                                        datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "                                                       )\n",
    "subm.to_csv(submission_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kg config -u $KAGGLE_USER -p $KAGGLE_PW -c $competition_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!kg submit $submission_file_name -m $description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
